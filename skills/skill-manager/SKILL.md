---
name: skill-manager
description: Comprehensive skill lifecycle management for creating, reviewing, improving, and testing Agent Skills. Use when users want to (1) create a new skill from scratch, (2) review an existing skill against best practices, (3) improve or refactor a skill based on feedback, error logs, or conversation history, (4) generate evaluation test cases for a skill. Triggers on keywords like 'skill', 'create skill', 'improve skill', 'review skill', 'skill quality', 'SKILL.md'. Do NOT use for general code review unrelated to skill development, nor for MCP server configuration, nor for general project instruction files (e.g. .cursorrules, CLAUDE.md).
---

# Skill Manager

Manage the full lifecycle of Agent Skills: creation, review, improvement, and testing.

## Mode Selection

Determine the appropriate workflow based on the user's request:

1. **New skill requested?** → Follow "Creation Workflow"
2. **Existing skill needs quality check?** → Follow "Review Workflow"
3. **Skill has issues or feedback exists?** → Follow "Improvement Workflow"
4. **Need to verify skill reliability?** → Follow "Testing Workflow"

If unclear, ask the user which mode to use.

---

## Creation Workflow

Create a new skill following a structured process. Take your time to do this thoroughly.

### Step 1: Understand the Skill

Gather concrete examples of how the skill will be used. Ask the user:

- What specific problem does this skill solve?
- What inputs does it receive and what outputs should it produce?
- What would a user say that should trigger this skill?
- What should NOT trigger this skill? (negative triggers)

Conclude when there is a clear sense of the functionality.

### Step 2: Plan Reusable Contents

Analyze each usage example to identify:

- **scripts/** — Code that would be rewritten repeatedly (deterministic operations)
- **references/** — Documentation needed while working (schemas, API docs, policies)
- **assets/** — Files used in output, not loaded into context (templates, images)

### Step 3: Initialize the Skill

Run the initialization script to scaffold the directory:

```bash
python scripts/init_skill.py <skill-name> --path <output-directory>
```

This creates the standard directory structure with template SKILL.md and example resources.

Skip this step if the skill directory already exists.

### Step 4: Implement the Skill

Build the skill contents following these principles:

1. **Start with reusable resources** — Implement scripts, references, and assets first
2. **Test scripts** by actually executing them to verify correctness
3. **Delete unneeded example files** generated by the initializer
4. **Write SKILL.md** following the frontmatter and body guidelines below

#### Frontmatter Guidelines

Write `name` and `description` fields only:

- **name**: Kebab-case, max 64 chars, `verb-gerund + noun` pattern preferred
- **description**: Max 1024 chars, third-person perspective, include:
  - What the skill does
  - Specific trigger keywords and file types
  - Negative triggers (what NOT to use it for)

See [best-practices.md](references/best-practices.md) for detailed description engineering guidance.

#### Body Guidelines

- Keep under **500 lines** — this is critical for context window efficiency
- Use imperative/infinitive form for instructions
- Only include knowledge the Agent doesn't already have
- Reference bundled resources with relative paths
- Keep references **1 level deep** (no nested file chains)

For structural patterns, consult:
- [workflows.md](references/workflows.md) — Sequential and conditional workflow patterns
- [output-patterns.md](references/output-patterns.md) — Template and example output patterns

### Step 5: Validate

Run the validation script to check structural correctness:

```bash
python scripts/quick_validate.py <path/to/skill-directory>
```

Fix any errors and re-run until validation passes.

### Step 6: Iterate

After the user tests the skill on real tasks, refine based on observed issues.

---

## Review Workflow

Evaluate an existing skill against best practices. Quality is more important than speed.

### Step 1: Load and Analyze

Read the target skill's SKILL.md and all bundled resources.

### Step 2: Run Validation

Execute `scripts/quick_validate.py` against the skill directory for structural checks.

### Step 3: Evaluate Against Checklist

Assess each dimension and assign a rating (✅ Pass / ⚠️ Warning / ❌ Fail):

| # | Dimension | Check |
|:--|:--|:--|
| 1 | **Frontmatter: name** | Kebab-case, ≤64 chars, no reserved words |
| 2 | **Frontmatter: description** | ≤1024 chars, third-person, includes triggers + negative triggers |
| 3 | **Body: line count** | ≤500 lines |
| 4 | **Body: conciseness** | No redundant general knowledge, focused on domain-specific content |
| 5 | **Body: reference depth** | All references 1 level from SKILL.md, no nested chains |
| 6 | **Body: freedom calibration** | Appropriate specificity for task fragility |
| 7 | **Scripts** | Tested, deterministic, no token-wasting code generation |
| 8 | **References** | Structured with TOC if >100 lines, no duplication with SKILL.md |
| 9 | **Assets** | Clearly separated from references, not loaded into context |
| 10 | **No extraneous files** | No README.md, CHANGELOG.md, INSTALLATION_GUIDE.md, etc. |

### Step 4: Report

Present findings in this format:

```markdown
## Skill Review: <skill-name>

### Summary
[1-2 sentence overall assessment]

### Findings
| # | Dimension | Rating | Detail |
|:--|:--|:--|:--|
| 1 | ... | ✅/⚠️/❌ | ... |

### Recommended Actions
1. [Priority-ordered actionable improvements]
```

---

## Improvement Workflow

Refine a skill based on real-world feedback. Do not skip validation steps.

### Step 1: Identify the Problem

Determine the issue category:

| Problem | Likely Cause | See |
|:--|:--|:--|
| Skill never triggers | Description too vague or missing keywords | best-practices.md §1 |
| Skill over-triggers | Description scope too broad | Add negative triggers |
| Inconsistent output | Freedom too high for the task type | best-practices.md §3 |
| Steps get skipped | Model laziness | best-practices.md §4 |
| Excessive token usage | SKILL.md too verbose or references duplicated | best-practices.md §2 |

### Step 2: Analyze Evidence

If the user provides conversation history, error logs, or examples of incorrect behavior:
1. Identify the exact point where the skill diverged from expected behavior
2. Trace the root cause to a specific section of SKILL.md or a bundled resource
3. Determine if the fix requires instruction changes, script updates, or structural refactoring

### Step 3: Apply Fixes

Common improvement patterns:

- **Trigger precision** → Refine description keywords and add negative triggers
- **Output consistency** → Add or tighten output templates (see [output-patterns.md](references/output-patterns.md))
- **Reliability** → Extract probabilistic natural-language instructions into deterministic scripts
- **Context efficiency** → Move verbose content from SKILL.md to references/
- **Laziness prevention** → Add explicit checkpoint instructions and encouragement phrases

### Step 4: Validate and Test

1. Run `scripts/quick_validate.py` to verify structural integrity
2. Test the modified skill with the original failing scenario
3. Test with edge cases to ensure the fix doesn't introduce regressions

---

## Testing Workflow

Define evaluation criteria and test cases for a skill.

### Step 1: Define Test Cases

For each key usage pattern, create a test case:

```json
{
  "name": "descriptive-test-name",
  "prompt": "User prompt that should trigger the skill",
  "test_files": ["optional/input/files"],
  "expected": {
    "format": "Expected output structure description",
    "assertions": [
      "Specific verifiable condition 1",
      "Specific verifiable condition 2"
    ]
  }
}
```

### Step 2: Include Edge Cases

Test at minimum:
- Normal usage (happy path)
- Boundary conditions (empty input, very large input)
- Negative cases (prompts that should NOT trigger the skill)
- Multiple trigger patterns from the description

### Step 3: Execute and Verify

Run each test case and verify:
1. Skill triggers correctly (or correctly does not trigger)
2. Output matches expected format and content
3. No unnecessary token consumption or file reads
4. Scripts execute without errors

### Step 4: Document Results

Record pass/fail status and any issues discovered for the improvement workflow.
